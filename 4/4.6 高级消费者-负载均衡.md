## 4.6 高级消费者-负载均衡

### 4.6.1 简介
Kafka的负载均衡是同一个ConsumerGroup下的不同Consumer线程可以动态消费Topic下的数据。当ConsumerGroup的成员发生变化时（有Consumer加入或者退出时），或者Topic下的Partition发生变化，都要重新分配Partition给存活的Consuemr。

前面我们说过Consumer启动时会在/consumers/group/ids/的ZK目录注册自己的id，Kafka集群内部Topic会在/brokers/topics/topic/的ZK目录下注册自己的Partition。Consumer一旦发现以上2个路径的数据发生变化时，就会触发Consumer的负载均衡流程。当Consumer发生负载均衡时，其主要步骤如下所示：
* 关闭所有的ConsumerFetcherThread
* 然后需要删除TopicPartition和ConsumerThread的映射关系，主要包括两部分：ZK上/consumers/group/owners/topic/partition目录下和数据和本地相关数据结构topicRegistry
* 然后重新分配该ConsumerGroup下不同Consumer内部的Consumer线程和Partition的映射关系
* 然后更新TopicPartition和ConsumerThread的映射关系，仍然是要更新ZK上的数据以及本地相关数据
* 最后重新启动所有ConsumerFetcherThread

我们在分析reinitializeConsumer方法时，说过Consumer会针对ZK的连接建立、Topic的Partition变化以及Consumer的增删分别建立3个不同Listener，分别是ZKSessionExpireListener、ZKTopicPartitionChangeListener和ZKRebalancerListener，不过最终都是通过ZkRebalancerListener的rebanlace方法完成具体的负载均衡流程的。

### 4.6.2 ZKRebalancerListener
我们直接看rebalance方法：
```scala
private var topicRegistry = new Pool[String, Pool[Int, PartitionTopicInfo]]

private def rebalance(cluster: Cluster): Boolean = {
    val myTopicThreadIdsMap = TopicCount.constructTopicCount(
        group, consumerIdString, zkUtils, config.excludeInternalTopics).getConsumerThreadIdsPerTopic
    //获取当前Kafka集群中在线的Broker列表
    val brokers = zkUtils.getAllBrokersInCluster()
    if (brokers.size == 0) {
        zkUtils.subscribeChildChanges(BrokerIdsPath, loadBalancerListener)
        true
    } else {
        //关闭ConsumerFetcherThread线程
        closeFetchers(cluster, kafkaMessageAndMetadataStreams, myTopicThreadIdsMap)
        if (consumerRebalanceListener != null) {
            consumerRebalanceListener.beforeReleasingPartitions(
                if (topicRegistry.size == 0)
                    new java.util.HashMap[String, java.util.Set[java.lang.Integer]]
                else
                    topicRegistry.map(topics =>
                        topics._1 -> topics._2.keys
                    ).toMap.asJava.asInstanceOf[java.util.Map[String, java.util.Set[java.lang.Integer]]]
            )
        }
        //删除TopicPartition和ConsumerThread的映射关系
        releasePartitionOwnership(topicRegistry)
        //重新分配ConsumerThread和Partition
        val assignmentContext = new AssignmentContext(group, consumerIdString, config.excludeInternalTopics, zkUtils)
        val globalPartitionAssignment = partitionAssignor.assign(assignmentContext)
        val partitionAssignment = globalPartitionAssignment.get(assignmentContext.consumerId)
        val currentTopicRegistry = new Pool[String, Pool[Int, PartitionTopicInfo]](
            valueFactory = Some((_: String) => new Pool[Int, PartitionTopicInfo]))

        //获取当前topicPartitions的offset
        val topicPartitions = partitionAssignment.keySet.toSeq
        val offsetFetchResponseOpt = fetchOffsets(topicPartitions)

        if (isShuttingDown.get || !offsetFetchResponseOpt.isDefined)
            false
        else {
            val offsetFetchResponse = offsetFetchResponseOpt.get
            //更新currentTopicRegistry，主要是重新创建TopicPartition对应的PartitionTopicInfo信息
            topicPartitions.foreach { case tp@TopicAndPartition(topic, partition) =>
                val offset = offsetFetchResponse.requestInfo(tp).offset
                val threadId = partitionAssignment(tp)
                addPartitionTopicInfo(currentTopicRegistry, partition, topic, offset, threadId)
            }

            //更新/consumers/group/owners/topic/partition路径下的映射关系
            if (reflectPartitionOwnershipDecision(partitionAssignment)) {
                allTopicsOwnedPartitionsCount = partitionAssignment.size
                topicRegistry = currentTopicRegistry

                //重新启动ConsumerFetcherThread线程
                updateFetcher(cluster)
                true
            } else {
                false
            }
        }
    }
}
```
这里简单分析一下该方法：
* 首先关闭所有的ConsumerFetcherThread
* 然后删除TopicPartition和ConsumerThread的映射关系：删除ZK上/consumers/group/owners/topic/partition路径下的数据并清理本地数据结构topicRegistry（TopicPartition和ConsumerThread的映射关系）
* 然后重新分配该ConsumerGroup下TopicPartition和ConsumerThread
* 然后更新本地数据结构topicRegistry，并更新ZK上/consumers/group/owners/topic/partition下的数据
* 最后重新启动所有ConsumerFetcherThread

接下来，我们会详细分析一下这几个步骤。

#### closeFetchers
我们首先看一下closeFetchers方法，该方法逻辑比较简单：主要是清理Queue，并shutdown所有的ConsumerFetcherThread。
```scala
private def closeFetchers(cluster: Cluster, messageStreams: Map[String, List[KafkaStream[_, _]]],
                          relevantTopicThreadIdsMap: Map[String, Set[ConsumerThreadId]]) {
    val queuesTobeCleared = topicThreadIdAndQueues.filter(q => relevantTopicThreadIdsMap.contains(q._1._1)).map(q => q._2)
    closeFetchersForQueues(cluster, messageStreams, queuesTobeCleared)
}
private def closeFetchersForQueues(cluster: Cluster,
                                   messageStreams: Map[String, List[KafkaStream[_, _]]],
                                   queuesToBeCleared: Iterable[BlockingQueue[FetchedDataChunk]]) {
    val allPartitionInfos = topicRegistry.values.map(p => p.values).flatten
    fetcher.foreach { f =>
        f.stopConnections()
        clearFetcherQueues(allPartitionInfos, cluster, queuesToBeCleared, messageStreams)
        if (config.autoCommitEnable) {
            commitOffsets(true)
        }
    }
}
def stopConnections() {
    if (leaderFinderThread != null) {
        leaderFinderThread.shutdown()
        leaderFinderThread = null
    }

    closeAllFetchers()

    partitionMap = null
    noLeaderPartitionSet.clear()
}
private def clearFetcherQueues(topicInfos: Iterable[PartitionTopicInfo], cluster: Cluster,
                               queuesTobeCleared: Iterable[BlockingQueue[FetchedDataChunk]],
                               messageStreams: Map[String, List[KafkaStream[_, _]]]) {

    // Clear all but the currently iterated upon chunk in the consumer thread's queue
    queuesTobeCleared.foreach(_.clear)
    info("Cleared all relevant queues for this fetcher")

    // Also clear the currently iterated upon chunk in the consumer threads
    if (messageStreams != null)
        messageStreams.foreach(_._2.foreach(s => s.clear()))
}
def closeAllFetchers() {
    lock synchronized {
        for ((_, fetcher) <- fetcherThreadMap) {
            fetcher.initiateShutdown()
        }

        for ((_, fetcher) <- fetcherThreadMap) {
            fetcher.shutdown()
        }
        fetcherThreadMap.clear()
    }
}
```

#### 清理TopicPartition和ConsumerThread的映射关系
关闭了ConsumerFetcherThread后，就直接调用releasePartitionOwnership方法清理TopicPartition和ConsumerThread的映射关系：
```scala
private def releasePartitionOwnership(localTopicRegistry: Pool[String, Pool[Int, PartitionTopicInfo]]) = {
    for ((topic, infos) <- localTopicRegistry) {
        for (partition <- infos.keys) {
            deletePartitionOwnershipFromZK(topic, partition)
        }
        localTopicRegistry.remove(topic)
    }
    allTopicsOwnedPartitionsCount = 0
}
```
该方法比较简单：就是遍历topicRegistry，从ZK上删除映射关系，并清空topicRegistry。

#### 重新分配TopicPartition和ConsumerThread
首先我们看一下AssignmentContext，它记录当前ConsumerGroup下的分配上下文信息，主要是每个Topic的订阅ConsumerThreads和Partitions。具体代码如下所示：
```scala
class AssignmentContext(group: String, val consumerId: String, excludeInternalTopics: Boolean, zkUtils: ZkUtils) {
    //当前Consumer消费的Topic和Thread
    val myTopicThreadIds: collection.Map[String, collection.Set[ConsumerThreadId]] = {
        val myTopicCount = TopicCount.constructTopicCount(group, consumerId, zkUtils, excludeInternalTopics)
        myTopicCount.getConsumerThreadIdsPerTopic
    }

    //map of topic -> consumers：该Group下Topic的订阅关系，从ZK目录/consumers/group/ids/上拉取
    val consumersForTopic: collection.Map[String, List[ConsumerThreadId]] =
        zkUtils.getConsumersPerTopic(group, excludeInternalTopics)

    //map of topic -> partitons：该Topic下的所有Partitions
    val partitionsForTopic: collection.Map[String, Seq[Int]] =
        zkUtils.getPartitionsForTopics(consumersForTopic.keySet.toSeq)

    val consumers: Seq[String] = zkUtils.getConsumersInGroup(group).sorted
}
```
其中consumersForTopic：以Topic为key，记录所有订阅该Topic的ConsumerThreads；partitionsForTopic：以Topic为key，记录该Topic下的所有Partitions。

接下来我们以RangeAssignor为例简单说明一下如何分配Partitions给ConsumerThreads：
```scala
class RangeAssignor() extends PartitionAssignor with Logging {

    def assign(ctx: AssignmentContext) = {
        val valueFactory = (_: String) => new mutable.HashMap[TopicAndPartition, ConsumerThreadId]
        val partitionAssignment =
            new Pool[String, mutable.Map[TopicAndPartition, ConsumerThreadId]](Some(valueFactory))
        //遍历当前Consumer订阅的每个Topic的Consumer线程
        for (topic <- ctx.myTopicThreadIds.keySet) {
            //取出当前ConsumerGroup下订阅这个Topic的所有Consumers
            val curConsumers = ctx.consumersForTopic(topic)
            //取出这个Topic的所有Partitions
            val curPartitions: Seq[Int] = ctx.partitionsForTopic(topic)

            //计算平均每个Consumer线程消费几个Partition
            val nPartsPerConsumer = curPartitions.size / curConsumers.size
            val nConsumersWithExtraPart = curPartitions.size % curConsumers.size

            //遍历该Topic下的所有Consumer线程：将该Topic下的所有Partition分配给这些线程
            for (consumerThreadId <- curConsumers) {
                val myConsumerPosition = curConsumers.indexOf(consumerThreadId)
                assert(myConsumerPosition >= 0)
                val startPart = nPartsPerConsumer * myConsumerPosition + myConsumerPosition.min(nConsumersWithExtraPart)
                val nParts = nPartsPerConsumer + (if (myConsumerPosition + 1 > nConsumersWithExtraPart) 0 else 1)

                if (nParts <= 0)
                    warn("No broker partitions consumed by consumer thread " + consumerThreadId + " for topic " + topic)
                else {
                    //记录TopicPartition和Consumer线程的映射关系
                    for (i <- startPart until startPart + nParts) {
                        val partition = curPartitions(i)
                        val assignmentForConsumer = partitionAssignment.getAndMaybePut(consumerThreadId.consumer)
                        assignmentForConsumer += (TopicAndPartition(topic, partition) -> consumerThreadId)
                    }
                }
            }
        }

        // assign Map.empty for the consumers which are not associated with topic partitions
        ctx.consumers.foreach(consumerId => partitionAssignment.getAndMaybePut(consumerId))
        partitionAssignment
    }
}
```

#### 更新topicRegistry
重新分配完TopicPartition和ConsumerThread之间的映射关系后，需要更新相关数据，首先需要更新TopicParition数据（offset），然后调用addPartitionTopicInfo重新创建PartitionTopicInfo并添加到本地topicRegistry中，然后调用reflectPartitionOwnershipDecision方法更新ZK上的相关数据。
```scala
private def addPartitionTopicInfo(currentTopicRegistry: Pool[String, Pool[Int, PartitionTopicInfo]],
                                  partition: Int, topic: String,
                                  offset: Long, consumerThreadId: ConsumerThreadId) {
    val partTopicInfoMap = currentTopicRegistry.getAndMaybePut(topic)

    val queue = topicThreadIdAndQueues.get((topic, consumerThreadId))
    val consumedOffset = new AtomicLong(offset)
    val fetchedOffset = new AtomicLong(offset)
    val partTopicInfo = new PartitionTopicInfo(topic,
        partition,
        queue,
        consumedOffset,
        fetchedOffset,
        new AtomicInteger(config.fetchMessageMaxBytes),
        config.clientId)
    partTopicInfoMap.put(partition, partTopicInfo)
    checkpointedZkOffsets.put(TopicAndPartition(topic, partition), offset)
}

private def reflectPartitionOwnershipDecision(partitionAssignment: Map[TopicAndPartition, ConsumerThreadId]): Boolean = {
    var successfullyOwnedPartitions: List[(String, Int)] = Nil
    val partitionOwnershipSuccessful = partitionAssignment.map { partitionOwner =>
        val topic = partitionOwner._1.topic
        val partition = partitionOwner._1.partition
        val consumerThreadId = partitionOwner._2
        val partitionOwnerPath = zkUtils.getConsumerPartitionOwnerPath(group, topic, partition)
        try {
            zkUtils.createEphemeralPathExpectConflict(partitionOwnerPath, consumerThreadId.toString)
            successfullyOwnedPartitions ::= (topic, partition)
            true
        } catch {
            case _: ZkNodeExistsException =>
                // The node hasn't been deleted by the original owner. So wait a bit and retry.
                false
        }
    }
    val hasPartitionOwnershipFailed = partitionOwnershipSuccessful.foldLeft(0)((sum, decision) => sum + (if (decision) 0 else 1))
    if (hasPartitionOwnershipFailed > 0) {
        // remove all paths that we have owned in ZK
        successfullyOwnedPartitions.foreach(topicAndPartition => deletePartitionOwnershipFromZK(topicAndPartition._1, topicAndPartition._2))
        false
    }
    else true
}
```

#### 重新启动ConsumerFetcherThread
最后重新启动ConsumerFetcherThread，开始拉取数据。
```scala
private def updateFetcher(cluster: Cluster) {
    // update partitions for fetcher
    var allPartitionInfos: List[PartitionTopicInfo] = Nil
    for (partitionInfos <- topicRegistry.values)
        for (partition <- partitionInfos.values)
            allPartitionInfos ::= partition

    fetcher.foreach(_.startConnections(allPartitionInfos, cluster))
}
```
