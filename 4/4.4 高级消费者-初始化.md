## 4.4 高级消费者-初始化

### 4.4.1 简介
High-level Consumer的实现基于ConsumerGroup：同一ConsumerGroup中只有一个Consumer会消费到某条消息（单播），而不同ConsumerGroup均可以消费到该消息（广播）。而且其不需要关心消息offset的处理，High-Level Consumer会自动将从某个Partition读取到的相关消息的offset存于Zookeeper中。

首先，我们看一个demo：
```Java
private void consume() {
    Properties properties = new Properties();
    properties.put("zookeeper.connect", "xxxxx");
    properties.put("group.id", "com.mi.iot.service.OpenhomeNotifyService");
    ...
    ConsumerConfig consumerConfig = new ConsumerConfig(properties);
    ConsumerConnector connector = Consumer.createJavaConsumerConnector(consumerConfig);

    Map<String, Integer> topicCountMap = new HashMap<>();
    String topic = "topic";
    topicCountMap.put(topic, 1);
    Map<String, List<KafkaStream<byte[], byte[]>>> consumers = connector.createMessageStreams(topicCountMap);
    List<KafkaStream<byte[], byte[]>> streams = consumers.get(topic);
    KafkaStream stream = streams.get(0);
    ConsumerIterator<byte[], byte[]> iterator = stream.iterator();
    while (iterator.hasNext()) {
        Log.info("message: ", new String(iterator.next().message()));
    }
}
```
接下来，我们将以这个demo为例简单讲解一下高级消费者的处理流程。

### 4.4.2 ConsumerConnector
我们先看一下ZookeeperConsumerConnector，一个Consumer会创建一个ZookeeperConsumerConnector，代表一个消费者进程。直接看该类：
```scala
private[kafka] class ZookeeperConsumerConnector(val config: ConsumerConfig, val enableFetcher: Boolean)
        extends ConsumerConnector with Logging with KafkaMetricsGroup {

    private var fetcher: Option[ConsumerFetcherManager] = None  //消息获取
    private var zkUtils: ZkUtils = null                         //与ZK通信
    private var topicRegistry = new Pool[String, Pool[Int, PartitionTopicInfo]] //topic注册信息
    private val checkpointedZkOffsets = new Pool[TopicAndPartition, Long]
    private val topicThreadIdAndQueues = new Pool[(String, ConsumerThreadId), BlockingQueue[FetchedDataChunk]]
    private val scheduler = new KafkaScheduler(threads = 1, threadNamePrefix = "kafka-consumer-scheduler-")
    private val messageStreamCreated = new AtomicBoolean(false)

    private var sessionExpirationListener: ZKSessionExpireListener = null
    private var topicPartitionChangeListener: ZKTopicPartitionChangeListener = null
    private var loadBalancerListener: ZKRebalancerListener = null

    private var offsetsChannel: BlockingChannel = null
    private val offsetsChannelLock = new Object

    private var wildcardTopicWatcher: ZookeeperTopicEventWatcher = null
    private var consumerRebalanceListener: ConsumerRebalanceListener = null

    ...
    //创建ZkUtils，其会创建对应的ZkConnection和ZkClient
    connectZk()
    //创建ConsumerFetcherManager
    createFetcher()
    //确保连接上offsetManager
    ensureOffsetManagerConnected()

    //启动定时提交offset线程
    if (config.autoCommitEnable) {
        scheduler.startup
        scheduler.schedule("kafka-consumer-autocommit",
            autoCommit _,
            delay = config.autoCommitIntervalMs,
            period = config.autoCommitIntervalMs,
            unit = TimeUnit.MILLISECONDS)
    }
    ....
}
```
简单分析一下该类，在创建ZookeeperConsumerConnector时，有几个初始化方法需要事先执行：
* Consumer要和ZK通信，所以会调用connectZk方法连接上ZooKeeper
* Consumer要消费数据，需要有FetcherThread，这里使用ConsumerFetcherManager统一管理所有的FetcherThread
* Consumer需要管理offset，而且Consumer可以消费多个topic的多个partition，这里使用OffsetManager统一管理offset

### 4.4.3 createMessageStreams
接下来我们直接看createMessageStreams方法：
```scala
def createMessageStreams[K, V](topicCountMap: Map[String, Int], keyDecoder: Decoder[K],
                               valueDecoder: Decoder[V]): Map[String, List[KafkaStream[K, V]]] = {
    consume(topicCountMap, keyDecoder, valueDecoder)
}
def consume[K, V](topicCountMap: scala.collection.Map[String, Int], keyDecoder: Decoder[K],
                  valueDecoder: Decoder[V]): Map[String, List[KafkaStream[K, V]]] = {
    ...
    val topicCount = TopicCount.constructTopicCount(consumerIdString, topicCountMap)

    //topic->ConsumerThreadIds
    val topicThreadIds = topicCount.getConsumerThreadIdsPerTopic

    // make a list of (queue,stream) pairs, one pair for each threadId
    val queuesAndStreams = topicThreadIds.values.map(threadIdSet =>
        threadIdSet.map(_ => {
            val queue = new LinkedBlockingQueue[FetchedDataChunk](config.queuedMaxMessages)
            val stream = new KafkaStream[K, V](queue, config.consumerTimeoutMs, keyDecoder, valueDecoder, config.clientId)
            (queue, stream)
        })
    ).flatten.toList

    val dirs = new ZKGroupDirs(config.groupId)
    //向ZK注册Consumer信息
    registerConsumerInZK(dirs, consumerIdString, topicCount)
    //初始化Consumer相关数据结构
    reinitializeConsumer(topicCount, queuesAndStreams)

    //返回KafkaStream，每个Topic对应多个KafkaStream，数量是TopicCount中的count
    loadBalancerListener.kafkaMessageAndMetadataStreams.asInstanceOf[Map[String, List[KafkaStream[K, V]]]]
}
```
简单分析一下该方法：
* 初始化相关数据结构topicThreadIds和queuesAndStreams等
* 向ZK注册Consumer信息：registerConsumerInZK
* 初始化Consumer相关数据：reinitializeConsumer

我们先看一下第一个：一个Consumer可以启动多个Thread来消费多个Topic，因此这里使用TopicThreadIds来保存其中的映射关系，该映射由以下方法完成，其中consumerIdString是当前Consumer在这个ConsumerGroup的编号，每个consumer在ConsumerGroup中的编号都是唯一的。
```scala
def getConsumerThreadIdsPerTopic = TopicCount.makeConsumerThreadIdsPerTopic(consumerIdString, topicCountMap)

def makeConsumerThreadIdsPerTopic(consumerIdString: String, topicCountMap: Map[String, Int]) = {
    val consumerThreadIdsPerTopicMap = new mutable。HashMap[String, Set[ConsumerThreadId]]()
    for ((topic, nConsumers) <- topicCountMap) {
        val consumerSet = new mutable.HashSet[ConsumerThreadId]
        for (i <- 0 until nConsumers)
            consumerSet += ConsumerThreadId(consumerIdString, i)
        consumerThreadIdsPerTopicMap.put(topic, consumerSet)
    }
    consumerThreadIdsPerTopicMap
}
```
这里使用一个例子说明一下：假设消费者C声明使用2个Thread消费topic1以及3个Thread消费topic2，则相关数据如下所示：
```
topicThreadIds = {
    topic1: [C_Thread1, C_Thread2],
    topic2: [C_Thread1, C_Thread2, C_Thread3]
}
queuesAndStreams = [
    (LinkedBlockingQueue_1，KafkaStream_1)，      //topic1:C_Thread1
    (LinkedBlockingQueue_2，KafkaStream_2)，      //topic1:C_Thread2
    (LinkedBlockingQueue_3，KafkaStream_3)，      //topic2:C_Thread1
    (LinkedBlockingQueue_4，KafkaStream_4)，      //topic2:C_Thread2
    (LinkedBlockingQueue_5，KafkaStream_5)，      //topic2:C_Thread3
]
```

接下来我们看一下Consumer信息在ZK的注册，registerConsumerInZK：前面分析Kafka在Zk的存储结构时，我们知道Consumer需要向ZK注册一个临时节点，路径为/consumers/group/ids/consumerId，内容为订阅的topic。简单看一下该方法：
```
private def registerConsumerInZK(dirs: ZKGroupDirs, consumerIdString: String, topicCount: TopicCount) {
    val timestamp = Time.SYSTEM.milliseconds.toString
    val consumerRegistrationInfo = Json.encodeAsString(Map("version" -> 1,
        "subscription" -> topicCount.getTopicCountMap.asJava,
        "pattern" -> topicCount.pattern,
        "timestamp" -> timestamp
    ).asJava)

    val zkWatchedEphemeral = new ZKCheckedEphemeral(dirs.consumerRegistryDir + "/" + consumerIdString,
        consumerRegistrationInfo,
        zkUtils.zkConnection.getZookeeper,
        false)
    zkWatchedEphemeral.create()
}
```

最后我们看一下Consumer相关数据的初始化，reinitializeConsumer，该方法比较复杂：
```scala
private def reinitializeConsumer[K, V](topicCount: TopicCount,
                                       queuesAndStreams: List[(LinkedBlockingQueue[FetchedDataChunk], KafkaStream[K, V])]) {

    val dirs = new ZKGroupDirs(config.groupId)

    // listener to consumer and partition changes
    if (loadBalancerListener == null) {
        val topicStreamsMap = new mutable.HashMap[String, List[KafkaStream[K, V]]]
        loadBalancerListener = new ZKRebalancerListener(
            config.groupId, consumerIdString, topicStreamsMap.asInstanceOf[scala.collection.mutable.Map[String, List[KafkaStream[_, _]]]])
    }

    // create listener for session expired event if not exist yet
    if (sessionExpirationListener == null)
        sessionExpirationListener = new ZKSessionExpireListener(dirs, consumerIdString, topicCount, loadBalancerListener)

    // create listener for topic partition change event if not exist yet
    if (topicPartitionChangeListener == null)
        topicPartitionChangeListener = new ZKTopicPartitionChangeListener(loadBalancerListener)

    // map of {topic -> List(stream)}：作为createMessageStreams的结果返回到应用程序
    val topicStreamsMap = loadBalancerListener.kafkaMessageAndMetadataStreams

    // map of {topic -> Set(thread-1, thread-2, ...)}
    val consumerThreadIdsPerTopic: Map[String, Set[ConsumerThreadId]] = topicCount.getConsumerThreadIdsPerTopic

    // list of (Queue, KafkaStream)
    val allQueuesAndStreams = topicCount match {
        case _: WildcardTopicCount =>
            (1 to consumerThreadIdsPerTopic.keySet.size).flatMap(_ => queuesAndStreams).toList
        case _: StaticTopicCount =>
            queuesAndStreams
    }

    val topicThreadIds = consumerThreadIdsPerTopic.map { case (topic, threadIds) =>
        threadIds.map((topic, _))
    }.flatten

    // pair of (topic, thread) and (queue, stream)
    val threadQueueStreamPairs = topicThreadIds.zip(allQueuesAndStreams)

    // 将topicThreadId和queue的映射关系保存到topicThreadIdAndQueues中
    threadQueueStreamPairs.foreach(e => {
        val topicThreadId = e._1
        val q = e._2._1
        topicThreadIdAndQueues.put(topicThreadId, q)
    })

    val groupedByTopic = threadQueueStreamPairs.groupBy(_._1._1)
    groupedByTopic.foreach(e => {
        val topic = e._1
        val streams = e._2.map(_._2._2).toList
        //完成topicStreamsMap的初始化，作为createMessageStreams的结果返回到应用程序
        topicStreamsMap += (topic -> streams)
    })

    // listener to consumer and partition changes
    zkUtils.subscribeStateChanges(sessionExpirationListener)
    zkUtils.subscribeChildChanges(dirs.consumerRegistryDir, loadBalancerListener)

    topicStreamsMap.foreach { topicAndStreams =>
        // register on broker partition path changes
        val topicPath = BrokerTopicsPath + "/" + topicAndStreams._1
        zkUtils.subscribeDataChanges(topicPath, topicPartitionChangeListener)
    }

    // explicitly trigger load balancing for this consumer
    loadBalancerListener.syncedRebalance()
}
```
简单分析一下该方法：
* 初始化ZKRebalancerListener、ZKSessionExpireListener和ZKTopicPartitionChangeListener并注册。其中ZKSessionExpireListener用于监听ZkSession状态变化事件
 ZKTopicPartitionChangeListener注册到/brokers/topics/topic监听其Data变更事件，在topic数据发生变化时调用；ZKRebalancerListener注册到/consumers/group/ids监听其Child变更事件，当该ConsumerGroup下的Consumer发生变化时调用
* 整理相关数据结构：将topicThreadId和queue的映射关系保存到topicThreadIdAndQueues中；完成topicStreamsMap的初始化，作为createMessageStreams的结果返回到应用程序
* 显式调用ZKRebalancerListener的syncRebalance方法

我们先看一下这三个Listener：ZKSessionExpireListener和ZKTopicPartitionChangeListener这两个Listener的逻辑比较简单：即当监听的数据发生变化时，则调用ZKRebalancerListener完成rebalance操作。
#### ZKSessionExpireListener
当Session失效时，新的会话建立时，立即进行rebalance操作。
```scala
class ZKSessionExpireListener(val dirs: ZKGroupDirs， val consumerIdString: String， val topicCount: TopicCount，
                              val loadBalancerListener: ZKRebalancerListener) extends IZkStateListener {
    def handleNewSession() {
        loadBalancerListener.resetState()
        registerConsumerInZK(dirs， consumerIdString， topicCount)
        loadBalancerListener.syncedRebalance()
    }
}
```
#### ZKTopicPartitionChangeListener
当topic的数据变化时，通过触发的方式启动rebalance操作。
```scala
class ZKTopicPartitionChangeListener(val loadBalancerListener: ZKRebalancerListener) extends IZkDataListener {
    def handleDataChange(dataPath : String， data: Object) {
        loadBalancerListener.rebalanceEventTriggered()
    }
}
```
关于rebalace操作，我们之后会详细分析，这里就不再详述。

最后我们看一下ZKRebalancerListener的syncedRebalance方法，关于rebalance操作这里不再详述：
```scala
class ZKRebalancerListener(val group: String, val consumerIdString: String,
                           val kafkaMessageAndMetadataStreams: mutable.Map[String, List[KafkaStream[_, _]]]) extends IZkChildListener {
    ....
    def syncedRebalance() {
        rebalanceLock synchronized {
            rebalanceTimer.time {
                for (i <- 0 until config.rebalanceMaxRetries) {
                    if (isShuttingDown.get()) {
                        return
                    }
                    var done = false
                    var cluster: Cluster = null
                    try {
                        cluster = zkUtils.getCluster()
                        //调用rebalance操作
                        done = rebalance(cluster)
                    } catch {
                        ...
                    }

                    if (done) {
                        return
                    }

                    // stop all fetchers and clear all the queues to avoid data duplication
                    closeFetchersForQueues(cluster, kafkaMessageAndMetadataStreams, topicThreadIdAndQueues.map(q => q._2))
                    Thread.sleep(config.rebalanceBackoffMs)
                }
            }
        }

        throw new ConsumerRebalanceFailedException(consumerIdString + " can't rebalance after " + config.rebalanceMaxRetries + " retries")
    }

    //之后我们会详细分析一下该方法，这里我们简单看一下updateFetcher方法：
    private def rebalance(cluster: Cluster): Boolean = {
        ...
        updateFetcher(cluster)
        ...
    }

    private def updateFetcher(cluster: Cluster) {
        //分配给Consumer的Partition列表（此时还不清楚Leader信息）
        var allPartitionInfos: List[PartitionTopicInfo] = Nil
        for (partitionInfos <- topicRegistry.values)
            for (partition <- partitionInfos.values)
                allPartitionInfos ::= partition
        fetcher.foreach(_.startConnections(allPartitionInfos, cluster))
    }
}
```
简单分析一下updateFetcher方法：
* allPartitionInfos是分配给Consumer的Partition列表(此时还不知道Leader信息)
* 此处的fetcher就是ZooKeeperConsumerConnector初始化时创建的ConsumerFetcherManager

### 4.4.4 ConsumerFetcherManager
接下来我们看一下ConsumerFetcherManager的startConnections方法：
```scala
def startConnections(topicInfos: Iterable[PartitionTopicInfo], cluster: Cluster) {
    leaderFinderThread = new LeaderFinderThread(consumerIdString + "-leader-finder-thread")
    leaderFinderThread.start()

    inLock(lock) {
        //partitionMap: map of TopicPartition -> PartitionTopicInfo
        partitionMap = topicInfos.map(tpi => (new TopicPartition(tpi.topic, tpi.partitionId), tpi)).toMap
        //noLeaderPartitionSet: set of TopicPartition
        noLeaderPartitionSet ++= topicInfos.map(tpi => new TopicPartition(tpi.topic, tpi.partitionId))
        cond.signalAll()
    }
}
```
该方法首先启动一个LeaderFinderThread，用于找到Topic-Partition的Leader。然后初始化partitionMap和noLeaderPartitionSet这两个数据结构：前者用于保存partition相关信息，后者用于保存还没有确定leader信息的partition。接下来我们看一下LeaderFinderThread的相关逻辑：
```scala
private class LeaderFinderThread(name: String) extends ShutdownableThread(name) {
    // thread responsible for adding the fetcher to the right broker when leader is available
    override def doWork() {
        val leaderForPartitionsMap = new HashMap[TopicPartition, BrokerEndPoint]
        lock.lock()
        try {
            while (noLeaderPartitionSet.isEmpty) {
                cond.await()
            }

            val brokers = ClientUtils.getPlaintextBrokerEndPoints(zkUtils)
            //获取TopicMetadata，其中包含partition的Leader信息
            val topicsMetadata = ClientUtils.fetchTopicMetadata(noLeaderPartitionSet.map(m => m.topic).toSet,
                brokers,
                config.clientId,
                config.socketTimeoutMs,
                correlationId.getAndIncrement).topicsMetadata
            topicsMetadata.foreach { tmd =>
                val topic = tmd.topic
                tmd.partitionsMetadata.foreach { pmd =>
                    val topicAndPartition = new TopicPartition(topic, pmd.partitionId)
                    if (pmd.leader.isDefined && noLeaderPartitionSet.contains(topicAndPartition)) {
                        val leaderBroker = pmd.leader.get
                        leaderForPartitionsMap.put(topicAndPartition, leaderBroker)
                        //从noLeaderPartitionSet中删除该TopicPartition
                        noLeaderPartitionSet -= topicAndPartition
                    }
                }
            }
        } catch {
            ...
        } finally {
            lock.unlock()
        }

        try {
            //为Partition创建或者分配一个FetcherThread
            addFetcherForPartitions(leaderForPartitionsMap.map { case (topicPartition, broker) =>
                topicPartition -> BrokerAndInitialOffset(broker, partitionMap(topicPartition).getFetchOffset())})
        } catch {
            ...
        }

        shutdownIdleFetcherThreads()
        Thread.sleep(config.refreshLeaderBackoffMs)
    }
}
def addFetcherForPartitions(partitionAndOffsets: Map[TopicPartition, BrokerAndInitialOffset]) {
    lock synchronized {
        //为partitions分配Fetcher：默认情况下一个Broker上的同一个Topic下的Partitions都是一个Fetcher
        val partitionsPerFetcher = partitionAndOffsets.groupBy { case (topicPartition, brokerAndInitialFetchOffset) =>
            BrokerAndFetcherId(brokerAndInitialFetchOffset.broker, getFetcherId(topicPartition.topic, topicPartition.partition))
        }

        for ((brokerAndFetcherId, initialFetchOffsets) <- partitionsPerFetcher) {
            val brokerIdAndFetcherId = BrokerIdAndFetcherId(brokerAndFetcherId.broker.id, brokerAndFetcherId.fetcherId)
            //fetcherThreadMap：map of BrokerIdAndFetcherId -> FetcherThread
            fetcherThreadMap.get(brokerIdAndFetcherId) match {
                case Some(f) if f.sourceBroker.host == brokerAndFetcherId.broker.host
                                    && f.sourceBroker.port == brokerAndFetcherId.broker.port =>
                // reuse the fetcher thread
                case Some(f) =>
                    f.shutdown()
                    addAndStartFetcherThread(brokerAndFetcherId, brokerIdAndFetcherId)
                case None =>
                    addAndStartFetcherThread(brokerAndFetcherId, brokerIdAndFetcherId)
            }

            //为FetcherThread添加其对应的Paritions信息
            fetcherThreadMap(brokerIdAndFetcherId).addPartitions(initialFetchOffsets.map { case (tp, brokerAndInitOffset) =>
                tp -> brokerAndInitOffset.initOffset
            })
        }

        def addAndStartFetcherThread(brokerAndFetcherId: BrokerAndFetcherId, brokerIdAndFetcherId: BrokerIdAndFetcherId) {
            val fetcherThread = createFetcherThread(brokerAndFetcherId.fetcherId, brokerAndFetcherId.broker)
            fetcherThreadMap.put(brokerIdAndFetcherId, fetcherThread)
            fetcherThread.start
        }
    }
}
private[server] def getFetcherId(topic: String, partitionId: Int): Int = {
    //numFetchersPerBroker默认为1
    lock synchronized {
        Utils.abs(31 * topic.hashCode() + partitionId) % numFetchersPerBroker
    }
}
```
简单看一下该方法：
* 首先通过获取TopicMetadata信息，获取partition的leader信息
* 然后调用addFetcherForPartitions方法为已确认Leader信息的Partition添加Fetcher线程，默认情况下同一个Broker上的一个Topic的Partitions使用同一个Fetcher
* 最后调用FetcherThread的addPartitions方法为该FetcherThread添加其对应的Partitions信息

### 4.4.5 总结
最后我们再总结一下相关数据结构的映射关系：
* Consumer可以使用多个Thread消费同一个Topic，每个Thread都会对应一个Queue，而一个Queue又会对应一个KafkaStream
* 关于Queue和Partition的对应关系，由Kafka的负载均衡系统负责，有两种分配算法：RoundRobin和Range分配算法，这个对应关系将保存到该Partition的PartitionTopicInfo中
* 默认情况下对于同一个Topic，一个Broker对应一个FetcherThread，这个Broker上的该Topic下的多个Partition都由这个FetcherThread负责拉取消息
* 拉取到消息后，根据PartitionTopicInfo可以找到该Partition对应的Queue，将消息添加到这个Queue即可
