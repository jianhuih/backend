## 4.5 高级消费者-拉取消息
本节我们看一下高级消费者的拉取消息的流程。

### 4.5.1 LeaderFinderThread
我们从LeaderFinderThread开始，首先看一下如何查找Partition的Leader信息：
```scala
private class LeaderFinderThread(name: String) extends ShutdownableThread(name) {
    // thread responsible for adding the fetcher to the right broker when leader is available
    override def doWork() {
        val leaderForPartitionsMap = new HashMap[TopicPartition, BrokerEndPoint]
        lock.lock()
        try {
            while (noLeaderPartitionSet.isEmpty) {
                cond.await()
            }

            val brokers = ClientUtils.getPlaintextBrokerEndPoints(zkUtils)
            //获取Partition对应的TopicMetadata
            val topicsMetadata = ClientUtils.fetchTopicMetadata(noLeaderPartitionSet.map(m => m.topic).toSet,
                brokers,
                config.clientId,
                config.socketTimeoutMs,
                correlationId.getAndIncrement).topicsMetadata
            topicsMetadata.foreach { tmd =>
                val topic = tmd.topic
                tmd.partitionsMetadata.foreach { pmd =>
                    val topicAndPartition = new TopicPartition(topic, pmd.partitionId)
                    if (pmd.leader.isDefined && noLeaderPartitionSet.contains(topicAndPartition)) {
                        val leaderBroker = pmd.leader.get
                        leaderForPartitionsMap.put(topicAndPartition, leaderBroker)
                        noLeaderPartitionSet -= topicAndPartition
                    }
                }
            }
        } catch {
            ...
        } finally {
            lock.unlock()
        }

        try {
            //为TopicPartition添加合适的FetcherThread
            addFetcherForPartitions(leaderForPartitionsMap.map { case (topicPartition, broker) =>
                topicPartition -> BrokerAndInitialOffset(broker, partitionMap(topicPartition).getFetchOffset())})
        } catch {
            ...
        }

        shutdownIdleFetcherThreads()
        Thread.sleep(config.refreshLeaderBackoffMs)
    }
}
```
简单分析一下doWork方法：
* 首先调用fetchTopicMetadata方法获取Parition的metadata，其中包含该Partition的Leader信息
* 然后调用addFetcherForPartitions方法为Partition添加合适的FetcherThread。

我们先看一下fetchTopicMetadata方法：
```scala
def fetchTopicMetadata(topics: Set[String], brokers: Seq[BrokerEndPoint], clientId: String, timeoutMs: Int,
                       correlationId: Int = 0): TopicMetadataResponse = {
    val props = new Properties()
    props.put("metadata.broker.list", brokers.map(_.connectionString).mkString(","))
    props.put("client.id", clientId)
    props.put("request.timeout.ms", timeoutMs.toString)
    val producerConfig = new ProducerConfig(props)
    fetchTopicMetadata(topics, brokers, producerConfig, correlationId)
}
def fetchTopicMetadata(topics: Set[String], brokers: Seq[BrokerEndPoint], producerConfig: ProducerConfig,
                       correlationId: Int): TopicMetadataResponse = {
    var fetchMetaDataSucceeded: Boolean = false
    var i: Int = 0
    val topicMetadataRequest = new TopicMetadataRequest(TopicMetadataRequest.CurrentVersion, correlationId, producerConfig.clientId, topics.toSeq)
    var topicMetadataResponse: TopicMetadataResponse = null
    var t: Throwable = null
    // shuffle the list of brokers before sending metadata requests so that most requests don't get routed to the same broker
    val shuffledBrokers = Random.shuffle(brokers)
    while (i < shuffledBrokers.size && !fetchMetaDataSucceeded) {
        val producer: SyncProducer = ProducerPool.createSyncProducer(producerConfig, shuffledBrokers(i))
        try {
            topicMetadataResponse = producer.send(topicMetadataRequest)
            fetchMetaDataSucceeded = true
        } catch {
            ...
        } finally {
            i = i + 1
            producer.close()
        }
    }
    ...
    topicMetadataResponse
}
```
简单分析一下该方法：这里使用SyncProducer向Brokers发送TopicMetadataRequest请求，Broker会响应TopicMetadataResponse，其中包含该Topic下所有的Partition信息：PartitionMetadata，其中包含该有Partition的Leader信息。

接下来我们看一下addFetcherForPartitions方法：
```scala
def addFetcherForPartitions(partitionAndOffsets: Map[TopicPartition, BrokerAndInitialOffset]) {
    lock synchronized {

        val partitionsPerFetcher = partitionAndOffsets.groupBy { case (topicPartition, brokerAndInitialFetchOffset) =>
            BrokerAndFetcherId(brokerAndInitialFetchOffset.broker, getFetcherId(topicPartition.topic, topicPartition.partition))
        }

        for ((brokerAndFetcherId, initialFetchOffsets) <- partitionsPerFetcher) {
            val brokerIdAndFetcherId = BrokerIdAndFetcherId(brokerAndFetcherId.broker.id, brokerAndFetcherId.fetcherId)
            fetcherThreadMap.get(brokerIdAndFetcherId) match {
                case Some(f) if f.sourceBroker.host == brokerAndFetcherId.broker.host
                                    && f.sourceBroker.port == brokerAndFetcherId.broker.port =>
                // reuse the fetcher thread
                case Some(f) =>
                    f.shutdown()
                    addAndStartFetcherThread(brokerAndFetcherId, brokerIdAndFetcherId)
                case None =>
                    addAndStartFetcherThread(brokerAndFetcherId, brokerIdAndFetcherId)
            }

            //调用FetcherThread的addPartitions方法Paritions信息添加到FetcherThread的partitionStates中
            fetcherThreadMap(brokerIdAndFetcherId).addPartitions(initialFetchOffsets.map { case (tp, brokerAndInitOffset) =>
                tp -> brokerAndInitOffset.initOffset
            })
        }

        def addAndStartFetcherThread(brokerAndFetcherId: BrokerAndFetcherId, brokerIdAndFetcherId: BrokerIdAndFetcherId) {
            val fetcherThread = createFetcherThread(brokerAndFetcherId.fetcherId, brokerAndFetcherId.broker)
            fetcherThreadMap.put(brokerIdAndFetcherId, fetcherThread)
            fetcherThread.start
        }
    }
}
```
前面我们已经分析过该方法，这里就不再详述，我们直接看addAndStartFetcherThread方法，其调用createFetcherThread方法创建一个FetcherThread，并启动。

### 4.5.2 ConsumerFetcherThread
这里以Consumer为例看一下ConsumerFetcherThread，我们简单看一下该类信息：
```scala
abstract class AbstractFetcherThread(name: String,
                                     clientId: String,
                                     val sourceBroker: BrokerEndPoint,
                                     fetchBackOffMs: Int = 0,
                                     isInterruptible: Boolean = true,
                                     includeLogTruncation: Boolean)
        extends ShutdownableThread(name, isInterruptible) {
    //保存Partitions信息，用于构造FetchRequest，在LeaderFinder创建该Thread时，调用addPartitions方法初始化该数据结构
    private[server] val partitionStates = new PartitionStates[PartitionFetchState]

    private def states() = partitionStates.partitionStates.asScala.map { state => state.topicPartition -> state.value }
    ...
}
class ConsumerFetcherThread(consumerIdString: String,
                            fetcherId: Int,
                            val config: ConsumerConfig,
                            sourceBroker: BrokerEndPoint,
                            partitionMap: Map[TopicPartition, PartitionTopicInfo],
                            val consumerFetcherManager: ConsumerFetcherManager)
        extends AbstractFetcherThread(name = s"ConsumerFetcherThread-$consumerIdString-$fetcherId-${sourceBroker.id}",
            clientId = config.clientId,
            sourceBroker = sourceBroker,
            fetchBackOffMs = config.refreshLeaderBackoffMs,
            isInterruptible = true,
            includeLogTruncation = false) {

    //simpleConsumer，用于发送FetchRequest拉取Partition数据
    private val simpleConsumer = new SimpleConsumer(sourceBroker.host, sourceBroker.port, config.socketTimeoutMs,
        config.socketReceiveBufferBytes, config.clientId)

    //FetchReqeustBuilder，用于构造FetchRequest
    private val fetchRequestBuilder = new FetchRequestBuilder().
            clientId(clientId).
            replicaId(Request.OrdinaryConsumerId).
            maxWait(config.fetchWaitMaxMs).
            minBytes(config.fetchMinBytes).
            requestVersion(3)
    ...
}
```

#### PartitionTopicInfo
ConsumerFetcherThread中有一个类成员partitionMap: Map[TopicPartition, PartitionTopicInfo]，这里简单看一下PartitionTopicInfo（很多地方都使用了该类）：
```scala
class PartitionTopicInfo(val topic: String,
                         val partitionId: Int,
                         private val chunkQueue: BlockingQueue[FetchedDataChunk],
                         private val consumedOffset: AtomicLong,
                         private val fetchedOffset: AtomicLong,
                         private val fetchSize: AtomicInteger,
                         private val clientId: String) extends Logging {

    private val consumerTopicStats = ConsumerTopicStatsRegistry.getConsumerTopicStat(clientId)

    def getConsumeOffset() = consumedOffset.get

    def getFetchOffset() = fetchedOffset.get

    def resetConsumeOffset(newConsumeOffset: Long) = {
        consumedOffset.set(newConsumeOffset)
    }

    def resetFetchOffset(newFetchOffset: Long) = {
        fetchedOffset.set(newFetchOffset)
    }

    def enqueue(messages: ByteBufferMessageSet) {
        val size = messages.validBytes
        if (size > 0) {
            val next = messages.shallowIterator.toSeq.last.nextOffset
            //将数据插入到chunkQueue
            chunkQueue.put(new FetchedDataChunk(messages, this, fetchedOffset.get))
            //更新fetchedOffset
            fetchedOffset.set(next)
            consumerTopicStats.getConsumerTopicStats(topic).byteRate.mark(size)
            consumerTopicStats.getConsumerAllTopicStats().byteRate.mark(size)
        } else if (messages.sizeInBytes > 0) {
            chunkQueue.put(new FetchedDataChunk(messages, this, fetchedOffset.get))
        }
    }
}
```
该类主要是保存Partition的相关信息，简单分析一下该类的成员信息：
```
chunkQueue：用于存储从该Partition获取到的数据
consumedOffset：已经消费的Offset
fetchedOffset：已经抓取到的Offset，每Fetch到数据后就需要更新该值
```
PartitionTopicInfo是在ZKRebalancerListener.rebalance时调用addPartitionTopicInfo创建的，如下所示：
```scala
private def addPartitionTopicInfo(currentTopicRegistry: Pool[String, Pool[Int, PartitionTopicInfo]],
                                  partition: Int, topic: String,
                                  offset: Long, consumerThreadId: ConsumerThreadId) {
    val partTopicInfoMap = currentTopicRegistry.getAndMaybePut(topic)
    //topicThreadIdAndQueues: map of topicThreadId -> queue
    val queue = topicThreadIdAndQueues.get((topic, consumerThreadId))
    val consumedOffset = new AtomicLong(offset)
    val fetchedOffset = new AtomicLong(offset)
    val partTopicInfo = new PartitionTopicInfo(topic,
        partition,
        queue,
        consumedOffset,
        fetchedOffset,
        new AtomicInteger(config.fetchMessageMaxBytes),
        config.clientId)
    partTopicInfoMap.put(partition, partTopicInfo)
    checkpointedZkOffsets.put(TopicAndPartition(topic, partition), offset)
}
```
此处的queue是在ConsumerConnector.consume方法中创建的，保存在ConsumerConnector的类成员topicThreadIdAndQueues中，前面我们说过一个Topic下的一个Thread对应一个Queue，所以会存在多个Partition对应一个Queue的情况。

#### doWork
接下来我们直接从AbstractFetcherThread的doWork方法开始：
```scala
//AbstractFetcherThread.scala
private def states() = partitionStates.partitionStates.asScala.map { state => state.topicPartition -> state.value }

override def doWork() {
    val fetchRequest = inLock(partitionMapLock) {
        //遍历partitionStates，构造FetchRequest
        val ResultWithPartitions(fetchRequest, partitionsWithError) = buildFetchRequest(states)
        if (fetchRequest.isEmpty) {
            partitionMapCond.await(fetchBackOffMs, TimeUnit.MILLISECONDS)
        }
        handlePartitionsWithErrors(partitionsWithError)
        fetchRequest
    }
    if (!fetchRequest.isEmpty)
        processFetchRequest(fetchRequest)
}
```
该方法比较简单，首先调用buildFetchRequest方法构造FetchRequest，然后调用processFetchRequest方法处理该FetchRequest。接下来我们看一下这两个方法：
```scala
//ConsumerFetcherThread.scala
protected def buildFetchRequest(partitionMap: collection.Seq[(TopicPartition, PartitionFetchState)])
                :ResultWithPartitions[FetchRequest] = {
    partitionMap.foreach { case ((topicPartition, partitionFetchState)) =>
        if (partitionFetchState.isReadyForFetch)
            fetchRequestBuilder.addFetch(topicPartition.topic, topicPartition.partition, partitionFetchState.fetchOffset, fetchSize)
    }

    ResultWithPartitions(new FetchRequest(fetchRequestBuilder.build()), Set())
}
```
该方法逻辑比较简单，就是使用fetchRequestBuilder遍历partitionStates中的partition数据构建一个FetchRequest。接下来我们看一下processFetchRequest方法：
```scala
//AbstractFetcherThread.scala
private def processFetchRequest(fetchRequest: REQ) {
    val partitionsWithError = mutable.Set[TopicPartition]()
    var responseData: Seq[(TopicPartition, PD)] = Seq.empty

    try {
        //获取数据
        responseData = fetch(fetchRequest)
    } catch {
        case t: Throwable =>
            if (isRunning) {
                inLock(partitionMapLock) {
                    partitionsWithError ++= partitionStates.partitionSet.asScala
                    partitionMapCond.await(fetchBackOffMs, TimeUnit.MILLISECONDS)
                }
            }
    }
    fetcherStats.requestRate.mark()

    if (responseData.nonEmpty) {
        // process fetched data
        inLock(partitionMapLock) {

            responseData.foreach { case (topicPartition, partitionData) =>
                val topic = topicPartition.topic
                val partitionId = topicPartition.partition
                Option(partitionStates.stateValue(topicPartition)).foreach(currentPartitionFetchState =>
                    //由于fetchRequest是基于partitionStates构造的，所以此处两者的offset应该是一致的
                    if (fetchRequest.offset(topicPartition) == currentPartitionFetchState.fetchOffset
                            && currentPartitionFetchState.isReadyForFetch) {
                        partitionData.error match {
                            case Errors.NONE =>
                                try {
                                    val records = partitionData.toRecords
                                    val newOffset = records.batches.asScala.lastOption.map(_.nextOffset).getOrElse(
                                            currentPartitionFetchState.fetchOffset)

                                    fetcherLagStats.getAndMaybePut(topic, partitionId).lag = Math.max(0L,
                                            partitionData.highWatermark - newOffset)
                                    //处理Partition数据
                                    processPartitionData(topicPartition, currentPartitionFetchState.fetchOffset, partitionData)

                                    val validBytes = records.validBytes
                                    if (validBytes > 0 && partitionStates.contains(topicPartition)) {
                                        //更新PartitionStates，主要是Partition的offset
                                        partitionStates.updateAndMoveToEnd(topicPartition, new PartitionFetchState(newOffset))
                                        fetcherStats.byteRate.mark(validBytes)
                                    }
                                } catch {
                                    ...
                                }
                            case Errors.OFFSET_OUT_OF_RANGE =>
                                try {
                                    //修正offsetOutOfRange的错误
                                    val newOffset = handleOffsetOutOfRange(topicPartition)
                                    partitionStates.updateAndMoveToEnd(topicPartition, new PartitionFetchState(newOffset))
                                } catch {
                                    ...
                                }
                            case Errors.NOT_LEADER_FOR_PARTITION =>    
                                partitionsWithError += topicPartition

                            case _ =>
                                partitionsWithError += topicPartition                            
                        }
                    })
            }
        }
    }

    if (partitionsWithError.nonEmpty) {
        handlePartitionsWithErrors(partitionsWithError)
    }
}
```
简单分析一下该方法：首先调用fetch方法获取Partition的数据，然后处理该PartitionData：
* PartitionData没有错误，则调用processPartitionData处理partition的数据，并更新相关partitionStates
* PartitionData中有OFFSET_OUT_OF_RANGE错误，则调用handleOffsetOutOfRange处理
* 其他错误则调用handlePartitionsWithErrors处理

接下来，我们会详细分析一下该流程，首先我们看一下fetch方法：
```scala
protected def fetch(fetchRequest: FetchRequest): Seq[(TopicPartition, PartitionData)] =
    simpleConsumer.fetch(fetchRequest.underlying).data.map { case (TopicAndPartition(t, p), value) =>
        new TopicPartition(t, p) -> new PartitionData(value)}
```
该方法比较简单，直接调用simpleConsumer的fetch方法拉取数据。
```scala
def processPartitionData(topicPartition: TopicPartition, fetchOffset: Long, partitionData: PartitionData) {
    val pti = partitionMap(topicPartition)
    if (pti.getFetchOffset != fetchOffset)
        throw new RuntimeException("Offset doesn't match for partition [%s,%d] pti offset: %d fetch offset: %d"
                .format(topicPartition.topic, topicPartition.partition, pti.getFetchOffset, fetchOffset))
    pti.enqueue(partitionData.underlying.messages.asInstanceOf[ByteBufferMessageSet])
}
```
该方法也比较简单，就是直接将该Partition的数据插入到对应的PartitionTopicInfo，其中保存有对应的BlockingQueue，其中对应着指定的KafkaStream。
```scala
def handleOffsetOutOfRange(topicPartition: TopicPartition): Long = {
    val startTimestamp = config.autoOffsetReset match {
        case OffsetRequest.SmallestTimeString => OffsetRequest.EarliestTime
        case _ => OffsetRequest.LatestTime
    }
    val topicAndPartition = TopicAndPartition(topicPartition.topic, topicPartition.partition)
    val newOffset = simpleConsumer.earliestOrLatestOffset(topicAndPartition, startTimestamp, Request.OrdinaryConsumerId)
    val pti = partitionMap(topicPartition)
    pti.resetFetchOffset(newOffset)
    pti.resetConsumeOffset(newOffset)
    newOffset
}
```
该方法也比较简单，根据Config.autoOffsetReset重置策略（Smallest则对应Earliest，Largest对应Latest），利用simpleConsumer发送OffsetRequest，获取到该Partition正确的offset，然后重新设置该Partition对应的PartitionTopicInfo中的offset信息。

### 4.5.4 KafkaStream
最后我们从应用程序端出发，看一下KafkaStream的消息迭代流程，我们直接看ConsumerIterator.makeNext方法：
```scala
class ConsumerIterator[K, V](private val channel: BlockingQueue[FetchedDataChunk],
                             consumerTimeoutMs: Int,
                             private val keyDecoder: Decoder[K],
                             private val valueDecoder: Decoder[V],
                             val clientId: String)
        extends IteratorTemplate[MessageAndMetadata[K, V]] with Logging {

    private val current: AtomicReference[Iterator[MessageAndOffset]] = new AtomicReference(null)
    private var currentTopicInfo: PartitionTopicInfo = null
    private var consumedOffset: Long = -1L
    private val consumerTopicStats = ConsumerTopicStatsRegistry.getConsumerTopicStat(clientId)

    ...

    protected def makeNext(): MessageAndMetadata[K, V] = {
        var currentDataChunk: FetchedDataChunk = null
        // if we don't have an iterator, get one
        var localCurrent = current.get()
        if (localCurrent == null || !localCurrent.hasNext) {
            //首先从Queue中取出当前的dataChunk
            if (consumerTimeoutMs < 0)
                currentDataChunk = channel.take
            else {
                currentDataChunk = channel.poll(consumerTimeoutMs, TimeUnit.MILLISECONDS)
                if (currentDataChunk == null) {
                    resetState()
                    throw new ConsumerTimeoutException
                }
            }

            if (currentDataChunk eq ZookeeperConsumerConnector.shutdownCommand) {
                return allDone
            } else {
                //从当前的dataChunk中获取对应的iterator
                currentTopicInfo = currentDataChunk.topicInfo
                val cdcFetchOffset = currentDataChunk.fetchOffset
                val ctiConsumeOffset = currentTopicInfo.getConsumeOffset
                if (ctiConsumeOffset < cdcFetchOffset) {
                    currentTopicInfo.resetConsumeOffset(cdcFetchOffset)
                }
                localCurrent = currentDataChunk.messages.iterator

                current.set(localCurrent)
            }

            // if we just updated the current chunk and it is empty that means the fetch size is too small!
            if (currentDataChunk.messages.validBytes == 0)
                throw new MessageSizeTooLargeException("Found a message larger than the maximum fetch size of this consumer xxxx")
        }

        //从当前dataChunk取出当前的message
        var item = localCurrent.next()
        //跳过已经消费过的message
        while (item.offset < currentTopicInfo.getConsumeOffset && localCurrent.hasNext) {
            item = localCurrent.next()
        }
        consumedOffset = item.nextOffset

        item.message.ensureValid()

        new MessageAndMetadata(currentTopicInfo.topic,
            currentTopicInfo.partitionId,
            item.message,
            item.offset,
            keyDecoder,
            valueDecoder,
            item.message.timestamp,
            item.message.timestampType)
    }
}
```
该方法比较简单，消息保存在BlockQueue中的DataChunk中，由于DataChunk又是一个消息集合，所以需要两层迭代：首先从Queue中取出来一个DataChunk，然后迭代该DataChunk。

### 4.5.5 总结
我们在梳理一下Consumer、Thread、Queue和FetcherThread之间的关系：
* Consumer可以订阅多个Topic，每个Topic可以使用多个Thread，而每个Thread将对应一个Queue（即KafkaStream）
* 而Queue和Partition的映射关系则有负载均衡算法确定：一个Queue可以对应多个Partition，而一个Partition只能对应一个Queue，该映射关系保存在PartitionTopicInfo中
* 一个Topic和Partition组合对应一个FetcherId，根据FetcherId和Broker可以创建一个FetcherThread，默认情况下一个Broker下同一个Topic的多个Partition使用同一个FetcherThread，即FetcherThread可能会对应多个Partition
* FetcherThread中保存有一个partitionMap成员，它是一个Map：TopicPartition -> PartitionTopicInfo。即保存了该FetcherThread负责的所有Partition的信息，
* FetcherThread拉取到数据后，则根据partitionMap获取到该Partition对应的PartitionTopicInfo，其中保存着该Partition对应的Queue，然后将数据插入到该Queue中，应用程序就可以从该KafkaStream消费到数据

简单总结一下Consumer端Fetch的流程：
* ZookeeperConsumerConnector.rebalance里会调用fetchOffsets方法，并初始化类成员topicRegistry表示消费者的topic注册信息
* topicRegistry中存放的是topic->partition->PartitionTopicInfo，而PartitionTopicInfo包括了fetchOffset和queue：其中fetchOffset则用来标识FetchThread从哪儿开始抓取Partition的数据，queue用来存储FetcherThread抓取到的数据
* FetcherThread遍历其中的partitionStates（该数据结构是LeaderFinder线程生成FetcherThread时初始化的）形成FetchRequest，通过SimpleConsumer抓取到PartitionData
* SimpleConsumer将FetchRequest发送到KafkaServer，由KafkaApis.handleFetchRequest处理抓取请求
* PartitionData是要返回给消费者的分区数据，在processPartitionData处理分区数据时，会填充到TopicPartitionInfo的队列中
* 现在queue阻塞队列有了数据, 而这个queue也用于KafkaStream
* 客户端通过迭代器不断获取/消费数据，实际上就是从queue中拉取消息
